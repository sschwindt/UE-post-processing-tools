# Post-Processing Tools for Niagara Fluids Simulations in Unreal Engine


This project provides post-processing tools for analyzing and visualizing fluid particle simulation data generated by Unreal Engine (UE) using the Niagara Fluids Plugin. It is tailored for the simulation of a vertical slot fishway (VSF) simulations.

**Key Features:**
- Automated parsing of simulation logfiles to extract relevant particle data
- Statistical analysis: average particle counts, velocity distributions, standard deviation per section
- High-quality plots of particle movement with customizable color mapping
- Configurable parameters for flexible analysis and visualization

---

## Requirements

### Python Version
- Python 3.7 or higher

### Dependencies

**Standard Libraries:**
- `os` -- Operating system interfaces
- `re` -- Regular expressions
- `csv` -- CSV file reading/writing
- `math` -- Mathematical operations

**Third-Party Libraries:**
- `numpy` -- Numerical computations
- `pandas` -- Data analysis
- `matplotlib` -- Plotting

Install dependencies with:

```bash
pip install numpy pandas matplotlib
```

> **Important:** On newest Ubuntu-based Linux Mint releases, pip only works within activated conda or virtual environments.

---

## Input Data Format

The tool expects a log file containing particle data in the following format:

```
LogBlueprintUserMessages KEY: VECTOR: X= Y= Z= VELOCITY POSITION
```
- Each section in the data file is separated by a `STOP` line.
- Lines not matching this format are ignored.

See `images/dataFileExample.png` for a sample.

> **Note:** Example log files can be downloaded from [Zenodo](https://zenodo.org/doi/10.5281/zenodo.15863135) (~8.2 GB).

---

## Quick Start and Usage

1. **Clone the repository:**

```bash
git clone https://github.com/sschwindt/UE-post-processing-tools.git
cd UE-post-processing-tools
```

2. **Configure the tool:**
   - Edit `config.py` to set:
     - `xs_limits`: X-axis limits for each cross-section
     - `Vmax`: Maximum value for the colorbar
     - `radius`: Point radius in scatterplots
     - `limit_Data`: Whether to exclude particles below a velocity threshold
     - `limit`: The velocity threshold value
     - `var1`, `var2`, `var3`: Global variables for auxiliary calculations

3. **Review *logfile_processory.py***
  - Check name of target logfile (`DEFAULT_LOGFILE`)
  - Verify `MODE` argument: `"normal"` plots x and y (i.e., z) axes of the cross section; `"qualitative"` removes all axis, which can be useful for qualitative comparisons with scalar fields produced with other software (e.g., ParaView/OpenFOAM).

4. **Place your `.log` file** in the project directory (or provide the path when running the script).

5. **Run the processor:**

```bash
python logfile_processor.py [your_logfile.log]
```
- If you provide a log file name as an argument, it will be used for processing.
- If you omit the argument, the default log file set in `config.py` will be used.

> **Note:** The code can be modified to not plot any axes by setting `mode = "normal"` to `mode = "qualitative"`. This was implemented to allow for comparison with the axis-less output of ParaView-processed OpenFOAM.
---

## Command-Line Usage

You can specify the log file to process directly from the command line:

```bash
python logfile_processor.py path/to/your_logfile.log
```

If no argument is given, the script will use the default log file path defined in `config.py`.

---

## Output

- **Plots:** If enabled in `config.py`, scatter plots are saved as PNG files (see `images/section_plot_1.png` for an example).
- **CSV:** Processed data is saved as `Values.csv`.
- **Console Output:** Key statistics and summaries are printed.

---

## Configuration

Edit `config.py` to customize behavior:

- `file_path`: Path to the input log file
- `xs_limits`: Dictionary mapping cross-section keys to (left, right) x-axis limits
- `Vmax`: Maximum velocity for color mapping
- `radius`: Scatter plot point size
- `limit_Data`: Boolean, whether to filter out low-velocity particles
- `limit`: Minimum velocity threshold
- `var1`, `var2`, `var3`: Auxiliary variables for custom calculations

---"

## Code Structure

- `logfile_processor.py`: Main driver; orchestrates parsing, analysis, and plotting
- `get_particle_attributes.py`: Extracts and processes particle data from log sections
- `plotter.py`: Handles plotting and visualization
- `config.py`: User-editable configuration
- `images/`: Example images and diagrams
- `Values.csv`: Output CSV (generated)

### Main Classes & Methods

#### `logfile_processor.LogFileProcessor`
- `__call__()`: Main execution; processes log, analyzes sections, generates plots
- `calculate_panda_values()`: Statistical analysis and CSV export
- `setup_section()`: Extracts and summarizes section data
- `create_pictures()`: Generates and saves plots
- `process_section()`: Handles per-section workflow

#### `get_particle_attributes.ParticleAttributeCalculator`
- `extract_sections()`: Splits file into sections
- `extract_valid_particles()`: Filters and parses valid particles
- `parse_line()`, `extract_vector_components()`, `extract_velocity()`: Data extraction helpers
- `calculate_average_velocity()`, `calculate_standard_deviation()`, `calculate_average_particle_count()`: Statistical calculations
- `determine_xs()`: Maps data to cross-section keys
- `sum_global_var()`: Auxiliary calculation using config variables

#### `plotter.Plotter`
- `__call__()`: Creates plots for given data
- `create_custom_cmap()`: Custom color map
- `plot_data()`, `setup_figure()`, `create_scatter_plot()`, `add_colorbar()`, `add_titles()`, `save_plot()`: Plotting utilities
- `reduce_particles()`: Filters by velocity
- `subtract_global_var()`: Auxiliary calculation using config variables

---

## Workflow Diagram

See `images/flowChart.png` for a visual overview of the code workflow.

---

## Contributing

Contributions are welcome! Please open issues or pull requests for bug fixes, improvements, or new features.

---

## License

This project is licensed under the MIT License. See `LICENSE` for details.

---

## Support

For questions or issues, please open an issue on GitHub or refer to the code comments and documentation above.
